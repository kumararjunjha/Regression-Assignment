                                            #Regression Assignment

### 1. What is Simple Linear Regression?
"""
Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (Y) and a single independent variable (X) using a linear equation: 

    Y = mX + c + e

where:
- **Y** is the dependent variable
- **X** is the independent variable
- **m** is the slope of the line
- **c** is the intercept
- **e** is the error term
"""

### 2. What are the key assumptions of Simple Linear Regression?
"""
1. **Linearity**: The relationship between X and Y should be linear.
2. **Independence**: The observations should be independent.
3. **Homoscedasticity**: Constant variance of residuals.
4. **Normality**: The residuals should be normally distributed.
"""

### 3. What does the coefficient m represent in the equation Y=mX+c?
"""
The coefficient **m** represents the slope of the line, indicating how much Y changes for a unit increase in X.
"""

### 4. What does the intercept c represent in the equation Y=mX+c?
"""
The intercept **c** represents the value of Y when X = 0.
"""

### 5. How do we calculate the slope m in Simple Linear Regression?
"""
The slope **m** is calculated using:

    m = (Σ(X - X̄)(Y - Ȳ)) / (Σ(X - X̄)²)
"""

### 6. What is the purpose of the least squares method in Simple Linear Regression?
"""
The **Least Squares Method** minimizes the sum of squared residuals to find the best-fitting regression line.
"""

### 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?
"""
R² measures the proportion of variance in Y explained by X. **Higher R²** means a better fit.
"""

### 8. What is Multiple Linear Regression?
"""
Multiple Linear Regression extends Simple Linear Regression to multiple independent variables:

    Y = b0 + b1X1 + b2X2 + ... + bnXn + e
"""

### 9. What is the main difference between Simple and Multiple Linear Regression?
"""
Simple Linear Regression has **one** independent variable, while Multiple Linear Regression has **two or more** independent variables.
"""

### 10. What are the key assumptions of Multiple Linear Regression?
"""
- **Linearity**
- **Independence**
- **Homoscedasticity**
- **No multicollinearity**
- **Normally distributed residuals**
"""

### 11. What is heteroscedasticity, and how does it affect results?
"""
Heteroscedasticity means non-constant variance in residuals, which can lead to unreliable coefficient estimates.
"""

### 12. How can you improve a Multiple Linear Regression model with high multicollinearity?
"""
- Remove correlated predictors
- Use **Principal Component Analysis (PCA)**
- Apply **Regularization (Ridge/Lasso Regression)**
"""

### 13. What are some common techniques for transforming categorical variables?
"""
- **One-Hot Encoding**
- **Label Encoding**
"""

### 14. What is the role of interaction terms in Multiple Linear Regression?
"""
Interaction terms capture the combined effect of two variables.
"""

### 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?
"""
In Simple Regression, the intercept is meaningful. In Multiple Regression, it may not be interpretable if features are not centered.
"""

### 16. What is the significance of the slope in regression analysis?
"""
The slope shows how much Y changes per unit increase in X.
"""

### 17. How does the intercept in a regression model provide context?
"""
The intercept represents the expected value of Y when all X variables are zero.
"""

### 18. What are the limitations of using R² as a measure of model performance?
"""
R² does not indicate if the model is overfitting or whether important variables are missing.
"""

### 19. How would you interpret a large standard error for a regression coefficient?
"""
A large standard error means the coefficient is unstable and may not be statistically significant.
"""

### 20. How can heteroscedasticity be identified in residual plots?
"""
Heteroscedasticity appears as a **funnel-shaped pattern** in residual plots.
"""

### 21. What does it mean if a Multiple Linear Regression model has high R² but low adjusted R²?
"""
It indicates overfitting due to unnecessary variables in the model.
"""

### 22. Why is it important to scale variables in Multiple Linear Regression?
"""
Scaling prevents features with large values from dominating the regression coefficients.
"""

### 23. What is polynomial regression?
"""
Polynomial regression models a nonlinear relationship between X and Y by introducing polynomial terms:

    Y = b0 + b1X + b2X² + ... + bnXⁿ + e
"""

### 24. How does polynomial regression differ from linear regression?
"""
Polynomial regression captures **nonlinear** relationships, whereas linear regression assumes a straight-line relationship.
"""

### 25. When is polynomial regression used?
"""
When data exhibits **curvature** that linear regression cannot capture.
"""

### 26. What is the general equation for polynomial regression?
"""
    Y = b0 + b1X + b2X² + b3X³ + ... + bnXⁿ + e
"""

### 27. Can polynomial regression be applied to multiple variables?
"""
Yes, by including polynomial terms for multiple predictors.
"""

### 28. What are the limitations of polynomial regression?
"""
- Prone to **overfitting**
- Requires careful **degree selection**
"""

### 29. What methods can be used to evaluate polynomial model fit?
"""
- **R² Score**
- **Cross-validation**
- **Residual analysis**
"""

### 30. Why is visualization important in polynomial regression?
"""
Visualization helps in understanding the **curvature and fit** of the model.
"""

### 31. How is polynomial regression implemented in Python?
```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

X = df[['X_column']]
y = df['Y_column']
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
model = LinearRegression().fit(X_poly, y)
